{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "from requests.exceptions import Timeout\n",
    "import concurrent.futures\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "from typing import List, Tuple, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global lock for file operations\n",
    "file_lock = threading.Lock()\n",
    "\n",
    "def download_single_file(mirna: str, download_dir: str, timeout: int = 30) -> Tuple[str, bool, str]:\n",
    "    \"\"\"Download a single miRNA file with better error handling\"\"\"\n",
    "    try:\n",
    "        url = f'https://rnasysu.com/encori/moduleDownload.php?source=agoClipRNA&type=xls&value=hg38;lncRNA;{mirna};1;0;0;1;None;all'\n",
    "        \n",
    "        # Add retry mechanism\n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = requests.get(url, timeout=timeout)\n",
    "                response.raise_for_status()\n",
    "                break\n",
    "            except (requests.exceptions.RequestException, Timeout) as e:\n",
    "                if attempt == max_retries - 1:\n",
    "                    raise e\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "        \n",
    "        filename = f\"{mirna.replace('/', '_')}.xls\"\n",
    "        file_path = os.path.join(download_dir, filename)\n",
    "        \n",
    "        with file_lock:\n",
    "            with open(file_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "        \n",
    "        return filename, True, \"\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return mirna, False, str(e)\n",
    "\n",
    "def read_file(file_path: str) -> Tuple[pd.DataFrame, bool]:\n",
    "    \"\"\"Optimized file reading with pandas\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read().strip()\n",
    "            \n",
    "        if \"No Available results.\" in content:\n",
    "            return pd.DataFrame(), True\n",
    "            \n",
    "        # Use pandas read_csv with tab delimiter\n",
    "        df = pd.read_csv(file_path, sep='\\t', comment='#', skip_blank_lines=True)\n",
    "        \n",
    "        if df.empty:\n",
    "            return df, False\n",
    "            \n",
    "        return df, False\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error reading file {file_path}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch_parallel(files_batch: List[str], download_dir: str, max_workers: int = 4) -> Tuple[pd.DataFrame, List[str], List[str]]:\n",
    "    \"\"\"Process files in parallel using ThreadPoolExecutor\"\"\"\n",
    "    no_results_files = []\n",
    "    error_files = []\n",
    "    processed_data = []\n",
    "    \n",
    "    def process_single_file(filename: str) -> Tuple[pd.DataFrame, str, bool]:\n",
    "        try:\n",
    "            file_path = os.path.join(download_dir, filename)\n",
    "            df, is_no_results = read_file(file_path)\n",
    "            \n",
    "            # Delete file after processing\n",
    "            with file_lock:\n",
    "                os.remove(file_path)\n",
    "                \n",
    "            if is_no_results:\n",
    "                return pd.DataFrame(), filename, True\n",
    "            \n",
    "            if not df.empty and 'miRNAname' in df.columns and 'geneName' in df.columns and 'geneType' in df.columns:\n",
    "                lncrna_mask = df['geneType'] == \"lncRNA\"\n",
    "                if lncrna_mask.any():\n",
    "                    return df[lncrna_mask][['miRNAname', 'geneName']], filename, False\n",
    "            \n",
    "            return pd.DataFrame(), filename, False\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {str(e)}\")\n",
    "            return pd.DataFrame(), filename, False\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(process_single_file, filename) for filename in files_batch]\n",
    "        \n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            df, filename, is_no_results = future.result()\n",
    "            \n",
    "            if is_no_results:\n",
    "                no_results_files.append(filename)\n",
    "            elif not df.empty:\n",
    "                processed_data.append(df)\n",
    "            else:\n",
    "                error_files.append(filename)\n",
    "    \n",
    "    # Combine all processed data efficiently\n",
    "    if processed_data:\n",
    "        final_df = pd.concat(processed_data, ignore_index=True)\n",
    "        final_df.columns = ['Column_B', 'Column_D']\n",
    "        return final_df, no_results_files, error_files\n",
    "    \n",
    "    return pd.DataFrame(columns=['Column_B', 'Column_D']), no_results_files, error_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_batch(mirnas: List[str], download_dir: str, max_workers: int = 8) -> Tuple[List[str], List[str], List[str]]:\n",
    "    \"\"\"Download a batch of miRNAs in parallel\"\"\"\n",
    "    downloaded_files = []\n",
    "    not_downloaded = []\n",
    "    errors = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_mirna = {executor.submit(download_single_file, mirna, download_dir): mirna for mirna in mirnas}\n",
    "        \n",
    "        for future in concurrent.futures.as_completed(future_to_mirna):\n",
    "            filename, success, error = future.result()\n",
    "            if success:\n",
    "                downloaded_files.append(filename)\n",
    "            else:\n",
    "                not_downloaded.append(filename)\n",
    "                if error:\n",
    "                    errors.append(f\"{filename}: {error}\")\n",
    "    \n",
    "    return downloaded_files, not_downloaded, errors\n",
    "\n",
    "def save_batch_results(batch_num: int, results: Dict) -> None:\n",
    "    \"\"\"Save batch results with error handling\"\"\"\n",
    "    os.makedirs('output', exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        for result_type, data in results.items():\n",
    "            if not data:\n",
    "                continue\n",
    "                \n",
    "            filename = f'output/{result_type}_batch_{batch_num}.csv'\n",
    "            if isinstance(data, pd.DataFrame):\n",
    "                data.to_csv(filename, index=False)\n",
    "            else:\n",
    "                pd.DataFrame({'Filename': data}).to_csv(filename, index=False)\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving batch {batch_num} results: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_process_mirna(excel_path: str, batch_size: int = 50, download_workers: int = 8, process_workers: int = 4) -> Tuple[int, int, int, int]:\n",
    "    \"\"\"Optimized main function with parallel processing\"\"\"\n",
    "    # Read input file efficiently\n",
    "    df = pd.read_excel(excel_path, usecols=['miRNA'])\n",
    "    miRNAs = df['miRNA'].dropna().unique()\n",
    "    \n",
    "    # Create directories\n",
    "    for dir_name in ['downloaded_files', 'output']:\n",
    "        os.makedirs(dir_name, exist_ok=True)\n",
    "    \n",
    "    # Process in batches with progress tracking\n",
    "    num_batches = math.ceil(len(miRNAs) / batch_size)\n",
    "    all_results = {\n",
    "        'data_not_found': [],\n",
    "        'not_downloaded': [],\n",
    "        'error_files': [],\n",
    "        'extracted_data': []\n",
    "    }\n",
    "    \n",
    "    with tqdm(total=num_batches, desc='Processing batches') as pbar:\n",
    "        for batch_num in range(num_batches):\n",
    "            start_idx = batch_num * batch_size\n",
    "            end_idx = min((batch_num + 1) * batch_size, len(miRNAs))\n",
    "            batch_miRNAs = miRNAs[start_idx:end_idx]\n",
    "            \n",
    "            # Download batch in parallel\n",
    "            downloaded_files, not_downloaded, errors = download_batch(\n",
    "                batch_miRNAs,\n",
    "                'downloaded_files',\n",
    "                max_workers=download_workers\n",
    "            )\n",
    "            \n",
    "            # Process downloaded files in parallel\n",
    "            extracted_data, no_results, process_errors = process_batch_parallel(\n",
    "                downloaded_files,\n",
    "                'downloaded_files',\n",
    "                max_workers=process_workers\n",
    "            )\n",
    "            \n",
    "            # Save batch results\n",
    "            batch_results = {\n",
    "                'data_not_found': no_results,\n",
    "                'not_downloaded': not_downloaded,\n",
    "                'error_files': errors + process_errors,\n",
    "                'extracted_data': extracted_data\n",
    "            }\n",
    "            save_batch_results(batch_num + 1, batch_results)\n",
    "            \n",
    "            # Accumulate results\n",
    "            for key in all_results:\n",
    "                if key == 'extracted_data':\n",
    "                    if not extracted_data.empty:\n",
    "                        all_results[key].append(extracted_data)\n",
    "                else:\n",
    "                    all_results[key].extend(batch_results[key])\n",
    "            \n",
    "            pbar.update(1)\n",
    "    \n",
    "    # Combine and save final results\n",
    "    if all_results['extracted_data']:\n",
    "        final_data = pd.concat(all_results['extracted_data'], ignore_index=True)\n",
    "        final_data.to_csv('output/extracted_data_final.csv', index=False)\n",
    "    \n",
    "    for key in ['data_not_found', 'not_downloaded', 'error_files']:\n",
    "        if all_results[key]:\n",
    "            pd.DataFrame({'Filename': all_results[key]}).to_csv(f'output/{key}_final.csv', index=False)\n",
    "    \n",
    "    # Clean up download directory\n",
    "    if os.path.exists('downloaded_files') and not os.listdir('downloaded_files'):\n",
    "        os.rmdir('downloaded_files')\n",
    "    \n",
    "    return (\n",
    "        len(all_results['data_not_found']),\n",
    "        len(final_data) if 'final_data' in locals() else 0,\n",
    "        len(all_results['error_files']),\n",
    "        len(all_results['not_downloaded'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
