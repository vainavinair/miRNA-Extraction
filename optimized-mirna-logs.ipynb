{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "from requests.exceptions import Timeout\n",
    "import concurrent.futures\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "from typing import List, Tuple, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import threading\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from logging.handlers import RotatingFileHandler\n",
    "\n",
    "# Global lock for file operations\n",
    "file_lock = threading.Lock()\n",
    "\n",
    "# Setup logging configuration\n",
    "def setup_logging():\n",
    "    \"\"\"Setup logging configuration with separate files for downloads and processing\"\"\"\n",
    "    logs_dir = \"logs\"\n",
    "    os.makedirs(logs_dir, exist_ok=True)\n",
    "    \n",
    "    # Create formatters and handlers\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    \n",
    "    # Download logger\n",
    "    download_logger = logging.getLogger('downloader')\n",
    "    download_logger.setLevel(logging.INFO)\n",
    "    download_handler = RotatingFileHandler(\n",
    "        os.path.join(logs_dir, 'downloads.log'),\n",
    "        maxBytes=1024*1024,  # 1MB\n",
    "        backupCount=5\n",
    "    )\n",
    "    download_handler.setFormatter(formatter)\n",
    "    download_logger.addHandler(download_handler)\n",
    "    \n",
    "    # Processing logger\n",
    "    process_logger = logging.getLogger('processor')\n",
    "    process_logger.setLevel(logging.INFO)\n",
    "    process_handler = RotatingFileHandler(\n",
    "        os.path.join(logs_dir, 'processing.log'),\n",
    "        maxBytes=1024*1024,  # 1MB\n",
    "        backupCount=5\n",
    "    )\n",
    "    process_handler.setFormatter(formatter)\n",
    "    process_logger.addHandler(process_handler)\n",
    "    \n",
    "    return download_logger, process_logger\n",
    "\n",
    "# Initialize loggers\n",
    "download_logger, process_logger = setup_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global lock for file operations\n",
    "file_lock = threading.Lock()\n",
    "\n",
    "def download_single_file(mirna: str, download_dir: str, timeout: int = 30) -> Tuple[str, bool, str]:\n",
    "    \"\"\"Download a single miRNA file with better error handling\"\"\"\n",
    "    try:\n",
    "        download_logger.info(f\"Starting download for miRNA: {mirna}\")\n",
    "        url = f'https://rnasysu.com/encori/moduleDownload.php?source=agoClipRNA&type=xls&value=hg38;lncRNA;{mirna};1;0;0;1;None;all'\n",
    "        \n",
    "        # Add retry mechanism\n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = requests.get(url, timeout=timeout)\n",
    "                response.raise_for_status()\n",
    "                break\n",
    "            except (requests.exceptions.RequestException, Timeout) as e:\n",
    "                if attempt == max_retries - 1:\n",
    "                    download_logger.error(f\"Failed to download {mirna} after {max_retries} attempts: {str(e)}\")\n",
    "                    raise e\n",
    "                download_logger.warning(f\"Retry {attempt + 1} for {mirna}\")\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "        \n",
    "        filename = f\"{mirna.replace('/', '_')}.xls\"\n",
    "        file_path = os.path.join(download_dir, filename)\n",
    "        \n",
    "        with file_lock:\n",
    "            with open(file_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "        \n",
    "        download_logger.info(f\"Successfully downloaded {mirna} to {filename}\")\n",
    "        return filename, True, \"\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        download_logger.error(f\"Error downloading {mirna}: {str(e)}\")\n",
    "        return mirna, False, str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified read_file function from test-new.ipynb (cell 2)\n",
    "def read_file(file_path: str) -> Tuple[pd.DataFrame, bool]:\n",
    "    \"\"\"Optimized file reading with manual content checking\"\"\"\n",
    "    try:\n",
    "        process_logger.info(f\"Reading file: {file_path}\")\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read().strip()\n",
    "            \n",
    "        if \"No Available results.\" in content:\n",
    "            process_logger.info(f\"No available results in {file_path}\")\n",
    "            return pd.DataFrame(), True\n",
    "            \n",
    "        # Manual processing from test-new.ipynb\n",
    "        lines = [line.strip() for line in content.split('\\n') if line.strip()]\n",
    "        data_lines = [line for line in lines if not line.startswith('#')]\n",
    "        \n",
    "        if not data_lines:\n",
    "            process_logger.warning(f\"No data lines found in {file_path}\")\n",
    "            return pd.DataFrame(), False\n",
    "        \n",
    "        processed_data = []\n",
    "        for line in data_lines:\n",
    "            row = [col.strip() for col in line.split('\\t') if col.strip()]\n",
    "            if row:\n",
    "                processed_data.append(row)\n",
    "        \n",
    "        if processed_data:\n",
    "            max_cols = max(len(row) for row in processed_data)\n",
    "            padded_data = [row + [''] * (max_cols - len(row)) for row in processed_data]\n",
    "            df = pd.DataFrame(\n",
    "                padded_data[1:], \n",
    "                columns=padded_data[0] if padded_data else [f'Column_{i}' for i in range(max_cols)]\n",
    "            )\n",
    "            process_logger.info(f\"Successfully processed {file_path}\")\n",
    "            return df, False\n",
    "\n",
    "        process_logger.warning(f\"No processed data found in {file_path}\")    \n",
    "        return pd.DataFrame(), False\n",
    "        \n",
    "    except Exception as e:\n",
    "        process_logger.error(f\"Error reading file {file_path}: {str(e)}\")\n",
    "        raise Exception(f\"Error reading file {file_path}: {str(e)}\")\n",
    "\n",
    "# Modified process_single_file logic from test-new.ipynb (cell 2)\n",
    "def process_batch_parallel(files_batch: List[str], download_dir: str, max_workers: int = 4) -> Tuple[pd.DataFrame, List[str], List[str]]:\n",
    "    \"\"\"Process files in parallel using ThreadPoolExecutor\"\"\"\n",
    "    no_results_files = []\n",
    "    error_files = []\n",
    "    processed_data = []\n",
    "    \n",
    "    def process_single_file(filename: str) -> Tuple[pd.DataFrame, str, bool]:\n",
    "        try:\n",
    "            process_logger.info(f\"Starting processing of {filename}\")\n",
    "            file_path = os.path.join(download_dir, filename)\n",
    "            df, is_no_results = read_file(file_path)\n",
    "            \n",
    "            with file_lock:\n",
    "                os.remove(file_path)\n",
    "                \n",
    "            if is_no_results:\n",
    "                return pd.DataFrame(), filename, True\n",
    "            \n",
    "            # Modified extraction logic from test-new.ipynb\n",
    "            if not df.empty and 'miRNAname' in df.columns and 'geneName' in df.columns and 'geneType' in df.columns:\n",
    "                lncrna_rows = df[df['geneType'] == \"lncRNA\"]\n",
    "                if not lncrna_rows.empty:\n",
    "                    temp_df = lncrna_rows[['miRNAname', 'geneName']]\n",
    "                    temp_df.columns = ['Column_B', 'Column_D']\n",
    "                    process_logger.info(f\"Successfully extracted data from {filename}\")\n",
    "                    return temp_df, filename, False\n",
    "            \n",
    "            process_logger.warning(f\"No valid data extracted from {filename}\")\n",
    "            return pd.DataFrame(), filename, False\n",
    "            \n",
    "        except Exception as e:\n",
    "            process_logger.error(f\"Error processing {filename}: {str(e)}\")\n",
    "            print(f\"Error processing {filename}: {str(e)}\")\n",
    "            return pd.DataFrame(), filename, False\n",
    "    \n",
    "    # Rest of the original threading logic remains the same\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(process_single_file, filename) for filename in files_batch]\n",
    "        \n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            df, filename, is_no_results = future.result()\n",
    "            \n",
    "            if is_no_results:\n",
    "                no_results_files.append(filename)\n",
    "            elif not df.empty:  # Explicit empty check\n",
    "                processed_data.append(df)\n",
    "            else:\n",
    "                error_files.append(filename)\n",
    "    \n",
    "    if processed_data:\n",
    "        final_df = pd.concat(processed_data, ignore_index=True)\n",
    "        return final_df, no_results_files, error_files\n",
    "    \n",
    "    return pd.DataFrame(columns=['Column_B', 'Column_D']), no_results_files, error_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_batch(mirnas: List[str], download_dir: str, max_workers: int = 8) -> Tuple[List[str], List[str], List[str]]:\n",
    "    \"\"\"Download a batch of miRNAs in parallel\"\"\"\n",
    "    downloaded_files = []\n",
    "    not_downloaded = []\n",
    "    errors = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_mirna = {executor.submit(download_single_file, mirna, download_dir): mirna for mirna in mirnas}\n",
    "        \n",
    "        for future in concurrent.futures.as_completed(future_to_mirna):\n",
    "            filename, success, error = future.result()\n",
    "            if success:\n",
    "                downloaded_files.append(filename)\n",
    "            else:\n",
    "                not_downloaded.append(filename)\n",
    "                if error:\n",
    "                    errors.append(f\"{filename}: {error}\")\n",
    "    \n",
    "    return downloaded_files, not_downloaded, errors\n",
    "\n",
    "def save_batch_results(batch_num: int, results: Dict) -> None:\n",
    "    \"\"\"Save batch results with error handling\"\"\"\n",
    "    os.makedirs('output', exist_ok=True)\n",
    "    try:\n",
    "        for result_type, data in results.items():\n",
    "            # Check if data is empty based on its type\n",
    "            if isinstance(data, pd.DataFrame):\n",
    "                if data.empty:\n",
    "                    continue  # Skip empty DataFrame\n",
    "            else:\n",
    "                if not data:\n",
    "                    continue  # Skip empty list\n",
    "            \n",
    "            filename = f'output/{result_type}_batch_{batch_num}.csv'\n",
    "            if isinstance(data, pd.DataFrame):\n",
    "                data.to_csv(filename, index=False)\n",
    "            else:\n",
    "                pd.DataFrame({'Filename': data}).to_csv(filename, index=False)\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving batch {batch_num} results: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_process_mirna(excel_path: str, batch_size: int = 50, download_workers: int = 8, process_workers: int = 4) -> Tuple[int, int, int, int]:\n",
    "    \"\"\"Optimized main function with parallel processing\"\"\"\n",
    "    # Read input file efficiently\n",
    "    df = pd.read_excel(excel_path, usecols=['miRNA'])\n",
    "    miRNAs = df['miRNA'].dropna().unique()\n",
    "    \n",
    "    # Create directories\n",
    "    for dir_name in ['downloaded_files', 'output']:\n",
    "        os.makedirs(dir_name, exist_ok=True)\n",
    "    \n",
    "    # Process in batches with progress tracking\n",
    "    num_batches = math.ceil(len(miRNAs) / batch_size)\n",
    "    all_results = {\n",
    "        'data_not_found': [],\n",
    "        'not_downloaded': [],\n",
    "        'error_files': [],\n",
    "        'extracted_data': []\n",
    "    }\n",
    "    \n",
    "    with tqdm(total=num_batches, desc='Processing batches') as pbar:\n",
    "        for batch_num in range(num_batches):\n",
    "            start_idx = batch_num * batch_size\n",
    "            end_idx = min((batch_num + 1) * batch_size, len(miRNAs))\n",
    "            batch_miRNAs = miRNAs[start_idx:end_idx]\n",
    "            \n",
    "            # Download batch in parallel\n",
    "            downloaded_files, not_downloaded, errors = download_batch(\n",
    "                batch_miRNAs,\n",
    "                'downloaded_files',\n",
    "                max_workers=download_workers\n",
    "            )\n",
    "            \n",
    "            # Process downloaded files in parallel\n",
    "            extracted_data, no_results, process_errors = process_batch_parallel(\n",
    "                downloaded_files,\n",
    "                'downloaded_files',\n",
    "                max_workers=process_workers\n",
    "            )\n",
    "            \n",
    "            # Save batch results\n",
    "            batch_results = {\n",
    "                'data_not_found': no_results,\n",
    "                'not_downloaded': not_downloaded,\n",
    "                'error_files': errors + process_errors,\n",
    "                'extracted_data': extracted_data\n",
    "            }\n",
    "            save_batch_results(batch_num + 1, batch_results)\n",
    "            \n",
    "            # Accumulate results\n",
    "            for key in all_results:\n",
    "                if key == 'extracted_data':\n",
    "                    if not extracted_data.empty:\n",
    "                        all_results[key].append(extracted_data)\n",
    "                else:\n",
    "                    all_results[key].extend(batch_results[key])\n",
    "            \n",
    "            pbar.update(1)\n",
    "    \n",
    "    # Combine and save final results\n",
    "    if all_results['extracted_data']:\n",
    "        final_data = pd.concat(all_results['extracted_data'], ignore_index=True)\n",
    "        final_data.to_csv('output/extracted_data_final.csv', index=False)\n",
    "    \n",
    "    for key in ['data_not_found', 'not_downloaded', 'error_files']:\n",
    "        if all_results[key]:\n",
    "            pd.DataFrame({'Filename': all_results[key]}).to_csv(f'output/{key}_final.csv', index=False)\n",
    "    \n",
    "    # Clean up download directory\n",
    "    if os.path.exists('downloaded_files') and not os.listdir('downloaded_files'):\n",
    "        os.rmdir('downloaded_files')\n",
    "    \n",
    "    return (\n",
    "        len(all_results['data_not_found']),\n",
    "        len(final_data) if 'final_data' in locals() else 0,\n",
    "        len(all_results['error_files']),\n",
    "        len(all_results['not_downloaded'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bc997e9db1a4148b6158ff07a44c224",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing complete!\n",
      "Files with no results: 168\n",
      "Files with errors: 0\n",
      "Files not downloaded (timeout): 0\n",
      "Total rows extracted: 1620\n"
     ]
    }
   ],
   "source": [
    "no_results_count, extracted_rows, error_count, not_downloaded_count = download_and_process_mirna(\n",
    "    'get_id.xlsx',\n",
    "    batch_size=50,\n",
    "    download_workers=8,  \n",
    "    process_workers=4    \n",
    ")\n",
    "print(f\"\\nProcessing complete!\")\n",
    "print(f\"Files with no results: {no_results_count}\")\n",
    "print(f\"Files with errors: {error_count}\")\n",
    "print(f\"Files not downloaded (timeout): {not_downloaded_count}\")\n",
    "print(f\"Total rows extracted: {extracted_rows}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
