{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "from requests.exceptions import Timeout\n",
    "import concurrent.futures\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "from typing import List, Tuple, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import threading\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from logging.handlers import RotatingFileHandler\n",
    "\n",
    "# Global lock for file operations\n",
    "file_lock = threading.Lock()\n",
    "\n",
    "# Setup logging configuration\n",
    "def setup_logging():\n",
    "    \"\"\"Setup logging configuration with separate files for downloads and processing\"\"\"\n",
    "    logs_dir = \"logs\"\n",
    "    os.makedirs(logs_dir, exist_ok=True)\n",
    "    \n",
    "    # Create formatters and handlers\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    \n",
    "    # Download logger\n",
    "    download_logger = logging.getLogger('downloader')\n",
    "    download_logger.setLevel(logging.INFO)\n",
    "    download_handler = RotatingFileHandler(\n",
    "        os.path.join(logs_dir, 'downloads.log'),\n",
    "        maxBytes=1024*1024,  # 1MB\n",
    "        backupCount=5\n",
    "    )\n",
    "    download_handler.setFormatter(formatter)\n",
    "    download_logger.addHandler(download_handler)\n",
    "    \n",
    "    # Processing logger\n",
    "    process_logger = logging.getLogger('processor')\n",
    "    process_logger.setLevel(logging.INFO)\n",
    "    process_handler = RotatingFileHandler(\n",
    "        os.path.join(logs_dir, 'processing.log'),\n",
    "        maxBytes=1024*1024,  # 1MB\n",
    "        backupCount=5\n",
    "    )\n",
    "    process_handler.setFormatter(formatter)\n",
    "    process_logger.addHandler(process_handler)\n",
    "    \n",
    "    return download_logger, process_logger\n",
    "\n",
    "# Initialize loggers\n",
    "download_logger, process_logger = setup_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global lock for file operations\n",
    "file_lock = threading.Lock()\n",
    "\n",
    "def download_single_file(mirna: str, download_dir: str, timeout: int = 30) -> Tuple[str, bool, str]:\n",
    "    \"\"\"Download a single miRNA file with better error handling\"\"\"\n",
    "    try:\n",
    "        download_logger.info(f\"Starting download for miRNA: {mirna}\")\n",
    "        url = f'https://rnasysu.com/encori/moduleDownload.php?source=agoClipRNA&type=xls&value=hg38;lncRNA;{mirna};1;0;0;1;None;all'\n",
    "        \n",
    "        # Add retry mechanism\n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = requests.get(url, timeout=timeout)\n",
    "                response.raise_for_status()\n",
    "                break\n",
    "            except (requests.exceptions.RequestException, Timeout) as e:\n",
    "                if attempt == max_retries - 1:\n",
    "                    download_logger.error(f\"Failed to download {mirna} after {max_retries} attempts: {str(e)}\")\n",
    "                    raise e\n",
    "                download_logger.warning(f\"Retry {attempt + 1} for {mirna}\")\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "        \n",
    "        filename = f\"{mirna.replace('/', '_')}.xls\"\n",
    "        file_path = os.path.join(download_dir, filename)\n",
    "        \n",
    "        with file_lock:\n",
    "            with open(file_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "        \n",
    "        download_logger.info(f\"Successfully downloaded {mirna} to {filename}\")\n",
    "        return filename, True, \"\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        download_logger.error(f\"Error downloading {mirna}: {str(e)}\")\n",
    "        return mirna, False, str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified read_file function from test-new.ipynb (cell 2)\n",
    "def read_file(file_path: str) -> Tuple[pd.DataFrame, bool]:\n",
    "    \"\"\"Optimized file reading with manual content checking\"\"\"\n",
    "    try:\n",
    "        process_logger.info(f\"Reading file: {file_path}\")\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read().strip()\n",
    "            \n",
    "        if \"No Available results.\" in content:\n",
    "            process_logger.info(f\"No available results in {file_path}\")\n",
    "            return pd.DataFrame(), True\n",
    "            \n",
    "        # Manual processing from test-new.ipynb\n",
    "        lines = [line.strip() for line in content.split('\\n') if line.strip()]\n",
    "        data_lines = [line for line in lines if not line.startswith('#')]\n",
    "        \n",
    "        if not data_lines:\n",
    "            process_logger.warning(f\"No data lines found in {file_path}\")\n",
    "            return pd.DataFrame(), False\n",
    "        \n",
    "        processed_data = []\n",
    "        for line in data_lines:\n",
    "            row = [col.strip() for col in line.split('\\t') if col.strip()]\n",
    "            if row:\n",
    "                processed_data.append(row)\n",
    "        \n",
    "        if processed_data:\n",
    "            max_cols = max(len(row) for row in processed_data)\n",
    "            padded_data = [row + [''] * (max_cols - len(row)) for row in processed_data]\n",
    "            df = pd.DataFrame(\n",
    "                padded_data[1:], \n",
    "                columns=padded_data[0] if padded_data else [f'Column_{i}' for i in range(max_cols)]\n",
    "            )\n",
    "            process_logger.info(f\"Successfully processed {file_path}\")\n",
    "            return df, False\n",
    "\n",
    "        process_logger.warning(f\"No processed data found in {file_path}\")    \n",
    "        return pd.DataFrame(), False\n",
    "        \n",
    "    except Exception as e:\n",
    "        process_logger.error(f\"Error reading file {file_path}: {str(e)}\")\n",
    "        raise Exception(f\"Error reading file {file_path}: {str(e)}\")\n",
    "\n",
    "# Modified process_single_file logic from test-new.ipynb (cell 2)\n",
    "def process_batch_parallel(files_batch: List[str], download_dir: str, max_workers: int = 4) -> Tuple[pd.DataFrame, List[str], List[str]]:\n",
    "    \"\"\"Process files in parallel using ThreadPoolExecutor\"\"\"\n",
    "    no_results_files = []\n",
    "    error_files = []\n",
    "    processed_data = []\n",
    "    \n",
    "    def process_single_file(filename: str) -> Tuple[pd.DataFrame, str, bool]:\n",
    "        try:\n",
    "            process_logger.info(f\"Starting processing of {filename}\")\n",
    "            file_path = os.path.join(download_dir, filename)\n",
    "            df, is_no_results = read_file(file_path)\n",
    "            \n",
    "            with file_lock:\n",
    "                os.remove(file_path)\n",
    "                \n",
    "            if is_no_results:\n",
    "                return pd.DataFrame(), filename, True\n",
    "            \n",
    "            # Modified extraction logic from test-new.ipynb\n",
    "            if not df.empty and 'miRNAname' in df.columns and 'geneName' in df.columns and 'geneType' in df.columns:\n",
    "                lncrna_rows = df[df['geneType'] == \"lncRNA\"]\n",
    "                if not lncrna_rows.empty:\n",
    "                    temp_df = lncrna_rows[['miRNAname', 'geneName']]\n",
    "                    temp_df.columns = ['Column_B', 'Column_D']\n",
    "                    process_logger.info(f\"Successfully extracted data from {filename}\")\n",
    "                    return temp_df, filename, False\n",
    "            \n",
    "            process_logger.warning(f\"No valid data extracted from {filename}\")\n",
    "            return pd.DataFrame(), filename, False\n",
    "            \n",
    "        except Exception as e:\n",
    "            process_logger.error(f\"Error processing {filename}: {str(e)}\")\n",
    "            print(f\"Error processing {filename}: {str(e)}\")\n",
    "            return pd.DataFrame(), filename, False\n",
    "    \n",
    "    # Rest of the original threading logic remains the same\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(process_single_file, filename) for filename in files_batch]\n",
    "        \n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            df, filename, is_no_results = future.result()\n",
    "            \n",
    "            if is_no_results:\n",
    "                no_results_files.append(filename)\n",
    "            elif not df.empty:  # Explicit empty check\n",
    "                processed_data.append(df)\n",
    "            else:\n",
    "                error_files.append(filename)\n",
    "    \n",
    "    if processed_data:\n",
    "        final_df = pd.concat(processed_data, ignore_index=True)\n",
    "        return final_df, no_results_files, error_files\n",
    "    \n",
    "    return pd.DataFrame(columns=['Column_B', 'Column_D']), no_results_files, error_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_batch(mirnas: List[str], download_dir: str, max_workers: int = 8) -> Tuple[List[str], List[str], List[str]]:\n",
    "    \"\"\"Download a batch of miRNAs in parallel\"\"\"\n",
    "    downloaded_files = []\n",
    "    not_downloaded = []\n",
    "    errors = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_mirna = {executor.submit(download_single_file, mirna, download_dir): mirna for mirna in mirnas}\n",
    "        \n",
    "        for future in concurrent.futures.as_completed(future_to_mirna):\n",
    "            filename, success, error = future.result()\n",
    "            if success:\n",
    "                downloaded_files.append(filename)\n",
    "            else:\n",
    "                not_downloaded.append(filename)\n",
    "                if error:\n",
    "                    errors.append(f\"{filename}: {error}\")\n",
    "    \n",
    "    return downloaded_files, not_downloaded, errors\n",
    "\n",
    "def save_batch_results(batch_num: int, results: Dict) -> None:\n",
    "    \"\"\"Save batch results with error handling\"\"\"\n",
    "    os.makedirs('output', exist_ok=True)\n",
    "    try:\n",
    "        for result_type, data in results.items():\n",
    "            # Check if data is empty based on its type\n",
    "            if isinstance(data, pd.DataFrame):\n",
    "                if data.empty:\n",
    "                    continue  # Skip empty DataFrame\n",
    "            else:\n",
    "                if not data:\n",
    "                    continue  # Skip empty list\n",
    "            \n",
    "            filename = f'output/{result_type}_batch_{batch_num}.csv'\n",
    "            if isinstance(data, pd.DataFrame):\n",
    "                data.to_csv(filename, index=False)\n",
    "            else:\n",
    "                pd.DataFrame({'Filename': data}).to_csv(filename, index=False)\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving batch {batch_num} results: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_process_mirna(excel_path: str, batch_size: int = 50, download_workers: int = 8, process_workers: int = 4) -> Tuple[int, int, int, int]:\n",
    "    \"\"\"Optimized main function with parallel processing\"\"\"\n",
    "    # Read input file efficiently\n",
    "    df = pd.read_excel(excel_path, usecols=['miRNA'])\n",
    "    miRNAs = df['miRNA'].dropna().unique()\n",
    "    \n",
    "    # Create directories\n",
    "    for dir_name in ['downloaded_files', 'output']:\n",
    "        os.makedirs(dir_name, exist_ok=True)\n",
    "    \n",
    "    # Process in batches with progress tracking\n",
    "    num_batches = math.ceil(len(miRNAs) / batch_size)\n",
    "    all_results = {\n",
    "        'data_not_found': [],\n",
    "        'not_downloaded': [],\n",
    "        'error_files': [],\n",
    "        'extracted_data': []\n",
    "    }\n",
    "    \n",
    "    with tqdm(total=num_batches, desc='Processing batches') as pbar:\n",
    "        for batch_num in range(num_batches):\n",
    "            start_idx = batch_num * batch_size\n",
    "            end_idx = min((batch_num + 1) * batch_size, len(miRNAs))\n",
    "            batch_miRNAs = miRNAs[start_idx:end_idx]\n",
    "            \n",
    "            # Download batch in parallel\n",
    "            downloaded_files, not_downloaded, errors = download_batch(\n",
    "                batch_miRNAs,\n",
    "                'downloaded_files',\n",
    "                max_workers=download_workers\n",
    "            )\n",
    "            \n",
    "            # Process downloaded files in parallel\n",
    "            extracted_data, no_results, process_errors = process_batch_parallel(\n",
    "                downloaded_files,\n",
    "                'downloaded_files',\n",
    "                max_workers=process_workers\n",
    "            )\n",
    "            \n",
    "            # Save batch results\n",
    "            batch_results = {\n",
    "                'data_not_found': no_results,\n",
    "                'not_downloaded': not_downloaded,\n",
    "                'error_files': errors + process_errors,\n",
    "                'extracted_data': extracted_data\n",
    "            }\n",
    "            save_batch_results(batch_num + 1, batch_results)\n",
    "            \n",
    "            # Accumulate results\n",
    "            for key in all_results:\n",
    "                if key == 'extracted_data':\n",
    "                    if not extracted_data.empty:\n",
    "                        all_results[key].append(extracted_data)\n",
    "                else:\n",
    "                    all_results[key].extend(batch_results[key])\n",
    "            \n",
    "            pbar.update(1)\n",
    "    \n",
    "    # Combine and save final results\n",
    "    if all_results['extracted_data']:\n",
    "        final_data = pd.concat(all_results['extracted_data'], ignore_index=True)\n",
    "        final_data.to_csv('output/extracted_data_final.csv', index=False)\n",
    "    \n",
    "    for key in ['data_not_found', 'not_downloaded', 'error_files']:\n",
    "        if all_results[key]:\n",
    "            pd.DataFrame({'Filename': all_results[key]}).to_csv(f'output/{key}_final.csv', index=False)\n",
    "    \n",
    "    # Clean up download directory\n",
    "    if os.path.exists('downloaded_files') and not os.listdir('downloaded_files'):\n",
    "        os.rmdir('downloaded_files')\n",
    "    \n",
    "    return (\n",
    "        len(all_results['data_not_found']),\n",
    "        len(final_data) if 'final_data' in locals() else 0,\n",
    "        len(all_results['error_files']),\n",
    "        len(all_results['not_downloaded'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "800d801d51754825858a3322528d5bf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m no_results_count, extracted_rows, error_count, not_downloaded_count \u001b[38;5;241m=\u001b[39m \u001b[43mdownload_and_process_mirna\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata.xlsx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocess_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m    \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mProcessing complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFiles with no results: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mno_results_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[13], line 27\u001b[0m, in \u001b[0;36mdownload_and_process_mirna\u001b[1;34m(excel_path, batch_size, download_workers, process_workers)\u001b[0m\n\u001b[0;32m     24\u001b[0m batch_miRNAs \u001b[38;5;241m=\u001b[39m miRNAs[start_idx:end_idx]\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Download batch in parallel\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m downloaded_files, not_downloaded, errors \u001b[38;5;241m=\u001b[39m \u001b[43mdownload_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_miRNAs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdownloaded_files\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_workers\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Process downloaded files in parallel\u001b[39;00m\n\u001b[0;32m     34\u001b[0m extracted_data, no_results, process_errors \u001b[38;5;241m=\u001b[39m process_batch_parallel(\n\u001b[0;32m     35\u001b[0m     downloaded_files,\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdownloaded_files\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     37\u001b[0m     max_workers\u001b[38;5;241m=\u001b[39mprocess_workers\n\u001b[0;32m     38\u001b[0m )\n",
      "Cell \u001b[1;32mIn[12], line 10\u001b[0m, in \u001b[0;36mdownload_batch\u001b[1;34m(mirnas, download_dir, max_workers)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mmax_workers) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m      8\u001b[0m     future_to_mirna \u001b[38;5;241m=\u001b[39m {executor\u001b[38;5;241m.\u001b[39msubmit(download_single_file, mirna, download_dir): mirna \u001b[38;5;28;01mfor\u001b[39;00m mirna \u001b[38;5;129;01min\u001b[39;00m mirnas}\n\u001b[1;32m---> 10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconcurrent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_completed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfuture_to_mirna\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuccess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msuccess\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32md:\\Work\\Projects\\RNA-script\\venv1\\Lib\\concurrent\\futures\\_base.py:243\u001b[0m, in \u001b[0;36mas_completed\u001b[1;34m(fs, timeout)\u001b[0m\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wait_timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    239\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\n\u001b[0;32m    240\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m (of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) futures unfinished\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[0;32m    241\u001b[0m                 \u001b[38;5;28mlen\u001b[39m(pending), total_futures))\n\u001b[1;32m--> 243\u001b[0m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m waiter\u001b[38;5;241m.\u001b[39mlock:\n\u001b[0;32m    246\u001b[0m     finished \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39mfinished_futures\n",
      "File \u001b[1;32md:\\Work\\Projects\\RNA-script\\venv1\\Lib\\threading.py:634\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    632\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 634\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32md:\\Work\\Projects\\RNA-script\\venv1\\Lib\\threading.py:334\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 334\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    335\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "no_results_count, extracted_rows, error_count, not_downloaded_count = download_and_process_mirna(\n",
    "    'data.xlsx',\n",
    "    batch_size=50,\n",
    "    download_workers=8,  \n",
    "    process_workers=4    \n",
    ")\n",
    "print(f\"\\nProcessing complete!\")\n",
    "print(f\"Files with no results: {no_results_count}\")\n",
    "print(f\"Files with errors: {error_count}\")\n",
    "print(f\"Files not downloaded (timeout): {not_downloaded_count}\")\n",
    "print(f\"Total rows extracted: {extracted_rows}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
