{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "from requests.exceptions import Timeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    \"\"\"Helper function to read files with proper column handling\"\"\"\n",
    "    try:\n",
    "        # Read file content\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read().strip()\n",
    "        \n",
    "        # Check for \"No Available results.\"\n",
    "        if \"No Available results.\" in content:\n",
    "            return None, True\n",
    "        \n",
    "        # Process file normally\n",
    "        lines = [line.strip() for line in content.split('\\n') if line.strip()]\n",
    "        data_lines = [line for line in lines if not line.startswith('#')]\n",
    "        \n",
    "        if not data_lines:\n",
    "            return pd.DataFrame(), False\n",
    "        \n",
    "        # Process lines and split by tabs\n",
    "        processed_data = []\n",
    "        for line in data_lines:\n",
    "            row = [col.strip() for col in line.split('\\t') if col.strip()]\n",
    "            if row:\n",
    "                processed_data.append(row)\n",
    "        \n",
    "        if processed_data:\n",
    "            max_cols = max(len(row) for row in processed_data)\n",
    "            padded_data = [row + [''] * (max_cols - len(row)) for row in processed_data]\n",
    "            df = pd.DataFrame(padded_data[1:], columns=padded_data[0] if padded_data else [f'Column_{i}' for i in range(max_cols)])\n",
    "            return df, False\n",
    "            \n",
    "        return pd.DataFrame(), False\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error reading file: {str(e)}\")\n",
    "\n",
    "def process_batch(files_batch, download_dir):\n",
    "    \"\"\"Process a batch of files and return extracted data and results\"\"\"\n",
    "    no_results_files = []\n",
    "    error_files = []\n",
    "    extracted_data = pd.DataFrame(columns=['Column_B', 'Column_D'])\n",
    "    \n",
    "    for filename in files_batch:\n",
    "        try:\n",
    "            file_path = os.path.join(download_dir, filename)\n",
    "            \n",
    "            # Process file\n",
    "            df, is_no_results = read_file(file_path)\n",
    "            \n",
    "            if is_no_results:\n",
    "                no_results_files.append(filename)\n",
    "            elif df is not None and not df.empty:\n",
    "                if 'miRNAname' in df.columns and 'geneName' in df.columns and 'geneType' in df.columns:\n",
    "                    lncrna_rows = df[df['geneType'] == \"lncRNA\"]\n",
    "                    if not lncrna_rows.empty:\n",
    "                        temp_df = lncrna_rows[['miRNAname', 'geneName']]\n",
    "                        temp_df.columns = ['Column_B', 'Column_D']\n",
    "                        extracted_data = pd.concat([extracted_data, temp_df], ignore_index=True)\n",
    "            \n",
    "            # Delete the processed file\n",
    "            os.remove(file_path)\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_files.append(filename)\n",
    "            print(f\"Error processing {filename}: {str(e)}\")\n",
    "    \n",
    "    return extracted_data, no_results_files, error_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_batch_results(batch_num, no_results, not_downloaded, errors, extracted_data):\n",
    "    \"\"\"Save results after each batch\"\"\"\n",
    "    os.makedirs('output', exist_ok=True)\n",
    "    \n",
    "    # Save no results\n",
    "    if no_results:\n",
    "        df = pd.DataFrame({'Filename': no_results})\n",
    "        df.to_csv(f'output/data_not_found_batch_{batch_num}.csv', index=False)\n",
    "        \n",
    "    # Save not downloaded\n",
    "    if not_downloaded:\n",
    "        df = pd.DataFrame({'Filename': not_downloaded})\n",
    "        df.to_csv(f'output/not_downloaded_batch_{batch_num}.csv', index=False)\n",
    "        \n",
    "    # Save errors\n",
    "    if errors:\n",
    "        df = pd.DataFrame({'Filename': errors})\n",
    "        df.to_csv(f'output/error_files_batch_{batch_num}.csv', index=False)\n",
    "        \n",
    "    # Save extracted data\n",
    "    if not extracted_data.empty:\n",
    "        extracted_data.to_csv(f'output/extracted_data_batch_{batch_num}.csv', index=False)\n",
    "\n",
    "def combine_batch_files():\n",
    "    \"\"\"Combine all batch files into final results\"\"\"\n",
    "    output_types = {\n",
    "        'data_not_found': [],\n",
    "        'not_downloaded': [],\n",
    "        'error_files': [],\n",
    "        'extracted_data': pd.DataFrame()\n",
    "    }\n",
    "    \n",
    "    for filename in os.listdir('output'):\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join('output', filename)\n",
    "            if 'data_not_found' in filename:\n",
    "                df = pd.read_csv(file_path)\n",
    "                output_types['data_not_found'].extend(df['Filename'].tolist())\n",
    "            elif 'not_downloaded' in filename:\n",
    "                df = pd.read_csv(file_path)\n",
    "                output_types['not_downloaded'].extend(df['Filename'].tolist())\n",
    "            elif 'error_files' in filename:\n",
    "                df = pd.read_csv(file_path)\n",
    "                output_types['error_files'].extend(df['Filename'].tolist())\n",
    "            elif 'extracted_data' in filename:\n",
    "                df = pd.read_csv(file_path)\n",
    "                output_types['extracted_data'] = pd.concat([output_types['extracted_data'], df], ignore_index=True)\n",
    "            \n",
    "            # Delete batch file after combining\n",
    "            os.remove(file_path)\n",
    "    \n",
    "    # Save final combined files\n",
    "    if output_types['data_not_found']:\n",
    "        pd.DataFrame({'Filename': output_types['data_not_found']}).to_csv('output/data_not_found_final.csv', index=False)\n",
    "    if output_types['not_downloaded']:\n",
    "        pd.DataFrame({'Filename': output_types['not_downloaded']}).to_csv('output/not_downloaded_final.csv', index=False)\n",
    "    if output_types['error_files']:\n",
    "        pd.DataFrame({'Filename': output_types['error_files']}).to_csv('output/error_files_final.csv', index=False)\n",
    "    if not output_types['extracted_data'].empty:\n",
    "        output_types['extracted_data'].to_csv('output/extracted_data_final.csv', index=False)\n",
    "    \n",
    "    return output_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def download_and_process_mirna(excel_path, batch_size=50):\n",
    "    \"\"\"Main function to download and process miRNA files in batches\"\"\"\n",
    "    # Read input Excel file\n",
    "    df = pd.read_excel(excel_path)\n",
    "    miRNAs = df['miRNA'].dropna().unique()\n",
    "    \n",
    "    # Create download directory\n",
    "    download_dir = 'downloaded_files'\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = 'output'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Initialize result storage for current batch\n",
    "    all_no_results = []\n",
    "    all_not_downloaded = []\n",
    "    all_errors = []\n",
    "    \n",
    "    # Calculate number of batches\n",
    "    num_batches = math.ceil(len(miRNAs) / batch_size)\n",
    "    \n",
    "    # Process in batches\n",
    "    for batch_num in tqdm(range(num_batches), desc='Processing batches'):\n",
    "        start_idx = batch_num * batch_size\n",
    "        end_idx = min((batch_num + 1) * batch_size, len(miRNAs))\n",
    "        batch_miRNAs = miRNAs[start_idx:end_idx]\n",
    "        \n",
    "        # Initialize batch results\n",
    "        batch_downloaded_files = []\n",
    "        batch_not_downloaded = []\n",
    "        \n",
    "        # Download batch\n",
    "        for miRNA in tqdm(batch_miRNAs, desc=f'Downloading batch {batch_num + 1}/{num_batches}', leave=False):\n",
    "            try:\n",
    "                url = f'https://rnasysu.com/encori/moduleDownload.php?source=agoClipRNA&type=xls&value=hg38;lncRNA;{miRNA};1;0;0;1;None;all'\n",
    "                response = requests.get(url, timeout=30)  # Add 30-second timeout\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                filename = f\"{miRNA.replace('/', '_')}.xls\"\n",
    "                file_path = os.path.join(download_dir, filename)\n",
    "                \n",
    "                with open(file_path, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "                batch_downloaded_files.append(filename)\n",
    "                \n",
    "            except Timeout:\n",
    "                print(f\"Timeout downloading {miRNA}\")\n",
    "                batch_not_downloaded.append(miRNA)\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading {miRNA}: {str(e)}\")\n",
    "                batch_not_downloaded.append(miRNA)\n",
    "        \n",
    "        # Process batch\n",
    "        extracted_data, no_results, errors = process_batch(batch_downloaded_files, download_dir)\n",
    "        \n",
    "        # Save batch results\n",
    "        save_batch_results(\n",
    "            batch_num + 1,\n",
    "            no_results,\n",
    "            batch_not_downloaded,\n",
    "            errors,\n",
    "            extracted_data\n",
    "        )\n",
    "        \n",
    "        # Accumulate results\n",
    "        all_no_results.extend(no_results)\n",
    "        all_not_downloaded.extend(batch_not_downloaded)\n",
    "        all_errors.extend(errors)\n",
    "    \n",
    "    # Combine all batch files into final results\n",
    "    final_results = combine_batch_files()\n",
    "    \n",
    "    # Clean up download directory if empty\n",
    "    if not os.listdir(download_dir):\n",
    "        os.rmdir(download_dir)\n",
    "    \n",
    "    return (\n",
    "        len(final_results['data_not_found']),\n",
    "        len(final_results['extracted_data']),\n",
    "        len(final_results['error_files']),\n",
    "        len(final_results['not_downloaded'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcb29a09ff39457abade6d6d5103def1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b6f4733f913447fbcdd2566c117e897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading batch 1/5:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout downloading hsa-miR-3167\n",
      "Timeout downloading hsa-miR-6752-5p\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f767fcbdb854c1790efb230cdb4a359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading batch 2/5:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout downloading hsa-miR-4722-3p\n",
      "Timeout downloading hsa-miR-3918\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "013260d4c5b641498e289eec7f0f3914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading batch 3/5:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be32b435438e4498bdc7940050258f94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading batch 4/5:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbfbcb40d5f743959a85fb12b90ab2ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading batch 5/5:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing complete!\n",
      "Files with no results: 166\n",
      "Files with errors: 0\n",
      "Files not downloaded (timeout): 4\n",
      "Total rows extracted: 1511\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Execute the combined process\n",
    "no_results_count, extracted_rows, error_count, not_downloaded_count = download_and_process_mirna('get_id.xlsx', batch_size=50)\n",
    "\n",
    "print(f\"\\nProcessing complete!\")\n",
    "print(f\"Files with no results: {no_results_count}\")\n",
    "print(f\"Files with errors: {error_count}\")\n",
    "print(f\"Files not downloaded (timeout): {not_downloaded_count}\")\n",
    "print(f\"Total rows extracted: {extracted_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
